# -*- coding: utf-8 -*-
"""IMDB Reviews Sentiment Analysis Using LSTM Neural Network, Deep Learning Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSjeUacMWw0-V3_zQK5Cj3McdgvtBuk8

**Importing kaggle library to download and access Data Sets**
"""

!pip install kaggle # to install kaggle library to import datasets

"""**Importing the Dependencies**"""

import os  #to interact with operating system,creating folders,setting fille paths
import json  #to read and write data in json format, kaggle api key that store credentials is also stored in json format

from zipfile import ZipFile  # importing utility that helps to unzip the compressed downloaded data sets
import pandas as pd  # for dataset loading and data analysis
from sklearn.model_selection import train_test_split  # to split data into train and test
from tensorflow.keras.models import Sequential  # to create sequential models that are linearly stacked as input and hidden and output layers
from tensorflow.keras.layers import Dense, Embedding, LSTM
# dense refers to fully connected neural network
# embedding help to convert words or tokens to numerical vector representation
# lstm model
from tensorflow.keras.preprocessing.text import Tokenizer  #to tokenize/split data into tokens base on words or sentences
from tensorflow.keras.preprocessing.sequence import pad_sequences  #ensures all input sequences have same shape

"""**Data Collection- Kaggle API**"""

kaggle_dictionary = json.load(open("kaggle.json")) # loading json format file,opening and saving in variable

kaggle_dictionary.keys()  # to check keys

# setup kaggle credentials as environment variables
os.environ["KAGGLE_USERNAME"] = kaggle_dictionary["username"] # os.environ is a dictiory object in python to access environment variables
os.environ["KAGGLE_KEY"] = kaggle_dictionary["key"] #

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

!ls  #to check list

# Unzip the dataset file

with ZipFile('/content/imdb-dataset-of-50k-movie-reviews.zip','r') as zip_ref:
  # with ZipFile utility we will provide path to unzip the zipped file, r commands to read a file and we will use alias as zip_ref
  zip_ref.extractall()  #extracting all the compressed data from zip file

!ls # cheking list of files on colab

"""**Loading Dataset**"""

df=pd.read_csv('/content/IMDB Dataset.csv')

df.shape

df.head()

df.tail()

df['sentiment'].value_counts()

df.replace({'sentiment':{'positive':1,'negative':0}},inplace=True)

df.head()

"""**Spliting the data into  train and test**"""

# For now I am going to split the whole dataframe into train data and tet data
train_data,test_data= train_test_split(df, test_size=0.2, random_state=42)
print(train_data.shape)
print(test_data.shape)

"""**Data Preprocessing**"""

# Tokenizing the text data
tokenizer= Tokenizer(num_words=5000)
#since we dont know vocablury size,so we take whole training data,num_words=5000 means tokenizer will select 5k most frequent words as vocablury and ignore all other words
# modern tokenizer not only just split text but also convert them to a unique integer representing the word
tokenizer.fit_on_texts(train_data['review']) #training tokenizer on direct texts data of review column
X_train= pad_sequences(tokenizer.texts_to_sequences(train_data['review']),maxlen=200)
X_test=pad_sequences(tokenizer.texts_to_sequences(test_data['review']),maxlen=200)
# The flow:
# 1) tokenizer.fit will train on text and get you the vocablury of most common 5000 words
# 2) then tokenizer.text_to_sequence will convert them into unique integers
# 3) then pad_sequence will ensure that each review/data point in review column has same length,
# 4) max_ len= 200 means every review/datapoint has 200 tokens equally
print(X_train)
print(X_test)

Y_train= train_data['sentiment']
Y_test= test_data['sentiment']

print(Y_train)
print(Y_test)

"""**Building Long Short Term Memory Model**"""

# Building the model

model= Sequential() #to stack layers linearly
model.add(Embedding(input_dim=5000,output_dim=128,input_length=200,input_shape=(200,))) # to convert data in its vector form, embedding should be the first layer of model
# input dimention represents 5000 most frequent selected tokens
# output dimention represents the vector representation or dimention of verctor,means each vector should be of 128 dimentions
# input length 200 reresents the max lenth of each datapoint,means each review should have equall 200 tokens
# input shape represents shape of layer
model.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))
# 128 represents 128 units,means each datapoint/review will be represented as 1 vector of 128 dimensions.
# dropout will randomly shutdown few neurons so that model donot overfit and generaize,in that way model donot reply on specific neurons/features
#drop out is applied to input/output layer whil recurrent drop out is applies to hidden layers
# droput= 0.2  represents that 20 percent of training data will be switch off or flow as 0..only applies during training.
#Flow:
#1) 5000 most frequent words will be converted into tokens and integers,integers just act like ids,embedding has the contexual meaning
#2) all these tokens will be converted into vectors, each vector has dimension of 128
#3) suppose we have 200 tokens so the shape will be then (200,128) means 200 tokens ,each token of 128 dimension
#4)then lstm(128) units means each data point will be then converted into 1 vector of 128 dimension
model.add(Dense(1, activation='sigmoid')) # dense shows fully connected neural network, 1 represents 1 out put neuron in output layer.

model.summary()

# none reresents batch size which is flexible and can be replaced by a any number
# batch size represents no. of samples being processed at once
# 200, 128 means each input has 200 tokens and each token is a 128 dimensional vector
# param represents parameters or trainable values

# Compiling the model
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
# optimizer changes weights and biases at every epochs for imporving prediction
# binary cross entropy loss is suitable for binary classification
# accuracy metrics to check model accuracy

"""**Model Training**"""

model.fit(X_train,Y_train,epochs=5,batch_size=64, validation_split=0.2)
# batch size represents no. of training samples being processed at once
# validation split =0.2 will reserve and use 20 percent of data for model assesement during training after every epochs to detect and prevent model from overfitting
# epochs are iterations, 1 epochs is 1 complet forward and backward pass

"""**Model Evaluation**"""

loss,accuracy= model.evaluate(X_test,Y_test)
print(f"The test loss is: {loss}")
print(f"Thes test accuracy is: {accuracy}")

"""**Building a Predictive System**"""

def predict_sentiment(review): # creating funtion name as predict_sequence that takes review as input
  sequence= tokenizer.texts_to_sequences(review) #converting text to tokens ang generating integers of input reviews
  paded_sequence= pad_sequences(sequence,maxlen=200) #will ensure every sequence/every review has equall 200 tokens
  prediction= model.predict(paded_sequence)
  sentiment= "Positive Review" if prediction[0][0]>0.5 else "Negative Review"  #we will have probailities [[0.5],[0.4]],not exact prediction like in ml
  # we will compare first and 2nd value with threashhold 0.5, if first value is greater then 0.5 means the review is positive,if 2nd value is greater means the review is negative
  return sentiment

#Example usage,calling the function
new_review= "this movie is great.I love the movie.Fantastic"
sentiment= predict_sentiment([new_review]) # the square bracket is necessary,not using them will let model take every word separatly as a separate input ,not the part of single review
# deep learning models are designed to do task in batches on large data set not one by one so it always expects list of reviews,multiple reviws or batches
# we will use [] to show it as a list even if we have only one data point or 1 instance of review
print(f" The Review is : {sentiment}")

#Example usage,calling the function
new_review= "this movie is bad.I hate the actors.not worth it"
sentiment= predict_sentiment([new_review])
print(f" The Review is : {sentiment}")

#Example usage,calling the function
new_review= "this movie is good"
sentiment= predict_sentiment([new_review])
print(f" The Review is : {sentiment}")

#Example usage,calling the function
new_review= "lovely"
sentiment= predict_sentiment([new_review])
print(f" The Review is : {sentiment}")

#Example usage,calling the function
new_review= "Better to stop making movies and wasting money"
sentiment= predict_sentiment([new_review])
print(f" The Review is : {sentiment}")